{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from torch import nn, optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"english10000.txt\",'r') as f:\n",
    "    words = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'and', 'to', 'a']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordset = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexdict = {None: 0}\n",
    "for i,w in enumerate(words):\n",
    "    indexdict[w] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexdict['center']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(txt):\n",
    "    txtwords = txt.lower().split()\n",
    "    dist = [0]*(len(words) + 1)\n",
    "    for w in txtwords:\n",
    "        if w in wordset:\n",
    "            dist[indexdict[w]] += 1\n",
    "        else:\n",
    "            if w.isalpha():\n",
    "                dist[0] += 1\n",
    "    tot = sum(dist)\n",
    "    return [d/tot for d in dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.14285714285714285,\n",
       " 0.2857142857142857,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_distribution(\"The of and to a adssssssssssssssss a\")[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://www.youtube.com/watch?v=qsXHcwe3krw|||h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>I'm finding the lack of me in these posts very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>Good one  _____   https://www.youtube.com/watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>Dear ,   I enjoyed our conversation the other ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>You're fired.|||That's another silly misconcep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  http://www.youtube.com/watch?v=qsXHcwe3krw|||h...\n",
       "1  ENTP  I'm finding the lack of me in these posts very...\n",
       "2  INTP  Good one  _____   https://www.youtube.com/watc...\n",
       "3  INTJ  Dear ,   I enjoyed our conversation the other ...\n",
       "4  ENTJ  You're fired.|||That's another silly misconcep..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti = pd.read_csv(\"kaggle/mbti.csv\")\n",
    "mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['type', 'posts'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([get_distribution(p) for p in mbti['posts']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(mbti['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=91, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf.pkl']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf,\"rf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['INFP'], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict([get_distribution(\"Hello World\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['INTP'], dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "\n",
    "Deep Learning Course Forums\n",
    "\n",
    "Problem creating custom loss function\n",
    "fastai users\n",
    " \n",
    " \n",
    "turntwo463\n",
    "David\n",
    "Mar 12\n",
    "I am trying to create and use a custom loss function. When my initial attempts failed I decided to take a step back and implement (through cut and paste) the standard loss function used with a unet Learner in my own notebook. I thought this would be a good way to check my understanding of the size of the tensor inputs and see where the inputs differed between the standard loss function and the ones I first created.\n",
    "\n",
    "To my disappointment my “cut and paste” loss function also does not work in that an exception is thrown during lr_find.\n",
    "\n",
    "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\n",
    "   1786     if input.size(0) != target.size(0):\n",
    "   1787         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n",
    "-> 1788                          .format(input.size(0), target.size(0)))\n",
    "   1789     if dim == 2:\n",
    "   1790         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
    "\n",
    "ValueError: Expected input batch_size (65536) to match target batch_size (8192).\n",
    "I would appreciate some insight into what I am doing wrong.\n",
    "\n",
    "Initial standard fastai code which does work:\n",
    "\n",
    "wd=1e-2\n",
    "learn = unet_learner(data, models.resnet34, metrics=[], wd=wd)\n",
    "print('Loss func ', learn.loss_func)\n",
    "Output:\n",
    "Loss func FlattenedLoss of CrossEntropyLoss()\n",
    "\n",
    "Here is the code I’ve pasted in (and renamed) that fails.\n",
    "\n",
    "class MyFlattenedLoss():\n",
    "    \"Same as `func`, but flattens input and target.\"\n",
    "    def __init__(self, func, *args, axis:int=-1, floatify:bool=False, is_2d:bool=True, **kwargs):\n",
    "        self.func,self.axis,self.floatify,self.is_2d = func(*args,**kwargs),axis,floatify,is_2d\n",
    "\n",
    "    def __repr__(self): return f\"My FlattenedLoss of {self.func}\"\n",
    "    @property\n",
    "    def reduction(self): return self.func.reduction\n",
    "    @reduction.setter\n",
    "    def reduction(self, v): self.func.reduction = v\n",
    "\n",
    "    def __call__(self, input:Tensor, target:Tensor, **kwargs)->Rank0Tensor:\n",
    "        print('input shape ', input.shape)\n",
    "        print('target shape ', target.shape)\n",
    "        \n",
    "        input = input.transpose(self.axis,-1).contiguous()\n",
    "        target = target.transpose(self.axis,-1).contiguous()\n",
    "        \n",
    "        print('input shape ', input.shape)\n",
    "        print('target shape ', target.shape)\n",
    "        \n",
    "        if self.floatify: target = target.float()\n",
    "        input = input.view(-1,input.shape[-1]) if self.is_2d else input.view(-1)\n",
    "        \n",
    "        print('input shape ', input.shape)\n",
    "        print('target shape ', target.shape)\n",
    "        print('floatify', self.floatify, ' 2d ', self.is_2d)\n",
    "        print('kwargs ', kwargs)\n",
    "        print('Func ', self.func)\n",
    "        print('target view ', target.view(-1).shape)\n",
    "        return self.func.__call__(input, target.view(-1), **kwargs)    \n",
    "    \n",
    "\n",
    "\n",
    "def MyCrossEntropyFlat(*args, axis:int=-1, **kwargs):\n",
    "    \"Same as `nn.CrossEntropyLoss`, but flattens input and target.\"\n",
    "    return MyFlattenedLoss(nn.CrossEntropyLoss, *args, axis=axis, **kwargs)\n",
    "\n",
    "wd=1e-2\n",
    "​learn = unet_learner(data, models.resnet34, metrics=[], wd=wd)\n",
    "learn.loss_func = MyCrossEntropyFlat()\n",
    "print('Loss func ', learn.loss_func)\n",
    "Output:\n",
    "Loss func My FlattenedLoss of CrossEntropyLoss()\n",
    "\n",
    "Exception occurs calling lr_find\n",
    "\n",
    "lr_find(learn)\n",
    "Note that the learner is setup to use a batch size of 8, there are 256 classes, and the images have been\n",
    "specified to be resized to [32,32]\n",
    "\n",
    "The following output is captured before the exception:\n",
    "\n",
    "input shape  torch.Size([8, 256, 32, 32])\n",
    "target shape  torch.Size([8, 1, 32, 32])\n",
    "input shape  torch.Size([8, 256, 32, 32])\n",
    "target shape  torch.Size([8, 1, 32, 32])\n",
    "input shape  torch.Size([65536, 32])\n",
    "target shape  torch.Size([8, 1, 32, 32])\n",
    "floatify False  2d  True\n",
    "kwargs  {}\n",
    "Func  CrossEntropyLoss()\n",
    "target view  torch.Size([8192])\n",
    "\n",
    "Reply\n",
    "\n",
    "created\n",
    "Mar 12\n",
    "last reply\n",
    "Mar 13\n",
    "2\n",
    "replies\n",
    "210\n",
    "views\n",
    "2\n",
    "users\n",
    "2\n",
    "likes\n",
    "\n",
    "\n",
    "renato\n",
    "Renato Hermoza\n",
    "Mar 12\n",
    "Try: learn.loss_func = MyCrossEntropyFlat(axis=1), thats the channel that indicates the labels.\n",
    "\n",
    "2\n",
    "\n",
    "Reply\n",
    "\n",
    "turntwo463\n",
    "David\n",
    "Mar 13\n",
    "Thank you! Specifying the axis index solved the issue.\n",
    "\n",
    "\n",
    "Reply\n",
    "  Bookmark   Share   Flag   Reply\n",
    " You will be notified if someone mentions your @name or replies to you.\n",
    "Suggested Topics\n",
    "Topic\tReplies\tViews\tActivity\n",
    "RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0 1\n",
    "fastai users\n",
    "6\t1.7k\t8d\n",
    "EfficientNet \n",
    "fastai users\n",
    "12\t74\t3h\n",
    "There are 41 unread and 29 new topics remaining, or browse other topics in \n",
    "fastai users\n",
    "\"\"\"\n",
    "rf.predict([get_distribution(txt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP', 'ESTJ', 'ESTP',\n",
       "       'INFJ', 'INFP', 'INTJ', 'INTP', 'ISFJ', 'ISFP', 'ISTJ', 'ISTP'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,X2,Y1,Y2 = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf4 = RandomForestClassifier(n_estimators=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=73, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf4.fit(X1,Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yp = rf4.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24438040345821327"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Yp,Y2,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
